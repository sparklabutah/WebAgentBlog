% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{zhouwebarena,
  title={WebArena: A Realistic Web Environment for Building Autonomous Agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024,
}

@inproceedings{koh2024visualwebarena,
  title={VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks},
  author={Koh, Jing Yu and Lo, Robert and Jang, Lawrence and Duvvur, Vikram and Lim, Ming Chong and Huang, Po-Yu and Neubig, Graham and Zhou, Shuyan and Salakhutdinov, Ruslan and Fried, Daniel},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024},
}

@article{deng2024mind2web,
  title={Mind2web: Towards a generalist agent for the web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{boisvert2024workarena++,
  title={Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks},
  author={Boisvert, L{\'e}o and Thakkar, Megh and Gasse, Maxime and Caccia, Massimo and Chezelles, De and Le Sellier, Thibault and Cappart, Quentin and Chapados, Nicolas and Lacoste, Alexandre and Drouin, Alexandre},
  journal={arXiv preprint arXiv:2407.05291},
  year={2024}
}

@article{pan2024webcanvas,
  title={WebCanvas: Benchmarking Web Agents in Online Environments},
  author={Pan, Yichen and Kong, Dehan and Zhou, Sida and Cui, Cheng and Leng, Yifei and Jiang, Bing and Liu, Hangyu and Shang, Yanyi and Zhou, Shuyan and Wu, Tongshuang and others},
  journal={arXiv preprint arXiv:2406.12373},
  year={2024}
}

@article{xie2024osworld,
  title={Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments},
  author={Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh Jing and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and others},
  journal={arXiv preprint arXiv:2404.07972},
  year={2024}
}

@article{kapoor2024omniact,
  title={OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web},
  author={Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and Alshikh, Waseem and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2402.17553},
  year={2024}
}

@article{rawles2024androidworld,
  title={AndroidWorld: A dynamic benchmarking environment for autonomous agents},
  author={Rawles, Christopher and Clinckemaillie, Sarah and Chang, Yifan and Waltz, Jonathan and Lau, Gabrielle and Fair, Marybeth and Li, Alice and Bishop, William and Li, Wei and Campbell-Ajala, Folawiyo and others},
  journal={arXiv preprint arXiv:2405.14573},
  year={2024}
}

@article{rawles2024androidinthewild,
  title={Androidinthewild: A large-scale dataset for android device control},
  author={Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{toyama2021androidenv,
  title={Androidenv: A reinforcement learning platform for android},
  author={Toyama, Daniel and Hamel, Philippe and Gergely, Anita and Comanici, Gheorghe and Glaese, Amelia and Ahmed, Zafarali and Jackson, Tyler and Mourad, Shibl and Precup, Doina},
  journal={arXiv preprint arXiv:2105.13231},
  year={2021}
}

@article{li2024effects,
  title={On the Effects of Data Scale on Computer Control Agents},
  author={Li, Wei and Bishop, William and Li, Alice and Rawles, Chris and Campbell-Ajala, Folawiyo and Tyamagundlu, Divya and Riva, Oriana},
  journal={arXiv preprint arXiv:2406.03679},
  year={2024}
}

@article{bonatti2024windows,
  title={Windows agent arena: Evaluating multi-modal os agents at scale},
  author={Bonatti, Rogerio and Zhao, Dan and Bonacci, Francesco and Dupont, Dillon and Abdali, Sara and Li, Yinheng and Wagle, Justin and Koishida, Kazuhito and Bucker, Arthur and Jang, Lawrence and others},
  journal={arXiv preprint arXiv:2409.08264},
  year={2024}
}

@article{wang2024opendevin,
  title={Opendevin: An open platform for ai software developers as generalist agents},
  author={Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},
  journal={arXiv preprint arXiv:2407.16741},
  year={2024}
}

@inproceedings{jimenezswe,
  title={SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024,
}

@misc{swebenchverifiedblog,
	author = {Neil Chowdhury and James Aung and Chan Jun Shern and Oliver Jaffe, Dane Sherburn and Giulio Starace and Evan Mays and Rachel Dias and Marwan Aljubeh and Mia Glaese and Carlos E. Jimenez and John Yang and Kevin Liu and Aleksander Madry},
	title = {{I}ntroducing {S}{W}{E}-bench {V}erified},
	howpublished = {\url{https://openai.com/index/introducing-swe-bench-verified/}},
	year = {2024},
}

@article{chen2024scienceagentbench,
  title={ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery},
  author={Chen, Ziru and Chen, Shijie and Ning, Yuting and Zhang, Qianheng and Wang, Boshi and Yu, Botao and Li, Yifei and Liao, Zeyi and Wei, Chen and Lu, Zitong and others},
  journal={arXiv preprint arXiv:2410.05080},
  year={2024}
}

@article{liu2024harnessing,
  title={Harnessing Webpage UIs for Text-Rich Visual Understanding},
  author={Liu, Junpeng and Ou, Tianyue and Song, Yifan and Qu, Yuxiao and Lam, Wai and Xiong, Chenyan and Chen, Wenhu and Neubig, Graham and Yue, Xiang},
  journal={arXiv preprint arXiv:2410.13824},
  year={2024}
}

@article{liu2024visualwebbench,
  title={VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?},
  author={Liu, Junpeng and Song, Yifan and Lin, Bill Yuchen and Lam, Wai and Neubig, Graham and Li, Yuanzhi and Yue, Xiang},
  journal={arXiv preprint arXiv:2404.05955},
  year={2024}
}

@inproceedings{pasupat-liang-2014-zero,
    title = "Zero-shot Entity Extraction from Web Pages",
    author = "Pasupat, Panupong  and
      Liang, Percy",
    editor = "Toutanova, Kristina  and
      Wu, Hua",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1037",
    doi = "10.3115/v1/P14-1037",
    pages = "391--401",
}


@InProceedings{pmlr-v70-shi17a,
  title = 	 {World of Bits: An Open-Domain Platform for Web-Based Agents},
  author =       {Tianlin Shi and Andrej Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3135--3144},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/shi17a/shi17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/shi17a.html},
  abstract = 	 {While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain realism of tasks in computer vision or natural language processing, which operate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions. The two main challenges are: (i) to curate a large, diverse set of interesting web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web. To do this, we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the web site. Finally, we show that agents trained via behavioral cloning and reinforcement learning can successfully complete a range of our web-based tasks.}
}

@inproceedings{liu2018reinforcement,
  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},
  author={Liu, Evan Zheran and Guu, Kelvin and Pasupat, Panupong and Shi, Tianlin and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{nogueira2016end,
  title={End-to-end goal-driven web navigation},
  author={Nogueira, Rodrigo and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{yoran2407assistantbench,
  title={Assistantbench: Can web agents solve realistic and time-consuming tasks?}, year={2024},
  author={Yoran, Ori and Amouyal, Samuel Joseph and Malaviya, Chaitanya and Bogin, Ben and Press, Ofir and Berant, Jonathan},
  journal={URL https://arxiv. org/abs/2407.15711}
}

@article{press2024citeme,
  title={CiteME: Can Language Models Accurately Cite Scientific Claims?},
  author={Press, Ori and Hochlehnert, Andreas and Prabhu, Ameya and Udandarao, Vishaal and Press, Ofir and Bethge, Matthias},
  journal={arXiv preprint arXiv:2407.12861},
  year={2024}
}

@article{mialon2023gaia,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}

@article{cao2024spider2,
  title={Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?},
  author={Cao, Ruisheng and Lei, Fangyu and Wu, Haoyuan and Chen, Jixuan and Fu, Yeqiao and Gao, Hongcheng and Xiong, Xinzhuang and Zhang, Hanchong and Mao, Yuchen and Hu, Wenjing and others},
  journal={arXiv preprint arXiv:2407.10956},
  year={2024}
}

@article{pan2024autonomous,
  title={Autonomous evaluation and refinement of digital agents},
  author={Pan, Jiayi and Zhang, Yichi and Tomlin, Nicholas and Zhou, Yifei and Levine, Sergey and Suhr, Alane},
  journal={arXiv preprint arXiv:2404.06474},
  year={2024}
}

@article{kapoor2024ai,
  title={Ai agents that matter},
  author={Kapoor, Sayash and Stroebl, Benedikt and Siegel, Zachary S and Nadgir, Nitya and Narayanan, Arvind},
  journal={arXiv preprint arXiv:2407.01502},
  year={2024}
}

@article{gou2024navigating,
  title={Navigating the digital world as humans do: Universal visual grounding for gui agents},
  author={Gou, Boyu and Wang, Ruohan and Zheng, Boyuan and Xie, Yanan and Chang, Cheng and Shu, Yiheng and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2410.05243},
  year={2024}
}

@inproceedings{agashe2024agent,
  title={Agent S: An Open Agentic Framework that Uses Computers Like a Human},
  author={Agashe, Saaket and Han, Jiuzhou and Gan, Shuyu and Yang, Jiachen and Li, Ang and Wang, Xin Eric},
  booktitle={NeurIPS 2024 Workshop on Open-World Agents},
  year={2024},
}

@misc{anthropicwebblog,
	author = {Anthropic},
	title = {Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku},
	howpublished = {https://www.anthropic.com/news/3-5-models-and-computer-use},
	year = {2024},
}

@misc{anthropicbuildingcublog,
	author = {Anthropic},
	title = {Developing a computer use model},
	howpublished = {https://www.anthropic.com/news/developing-computer-use},
	year = {2024},
}

@misc{projectmariner,
	author = {Google DeepMind},
	title = {Project Mariner},
	howpublished = {https://deepmind.google/technologies/project-mariner/},
	year = {2024},
}

@misc{computeruseagent,
	author = {OpenAI},
	title = {Project Mariner},
	howpublished = {https://openai.com/index/computer-using-agent/},
	year = {2025},
}


@misc{webaim,
	title = {WebAIM. The WebAIM Million.},
        key = {WebAIM},
	howpublished = {https://webaim.org/projects/million/},
	year = {2024},
}

@inproceedings{zhenggpt,
  title={GPT-4V (ision) is a Generalist Web Agent, if Grounded},
  year={2024},
  author={Zheng, Boyuan and Gou, Boyu and Kil, Jihyung and Sun, Huan and Su, Yu},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{sarch2024ical,
  title={Ical: Continual learning of multimodal agents by transforming trajectories into actionable insights},
  author={Sarch, Gabriel and Jang, Lawrence and Tarr, Michael J and Cohen, William W and Marino, Kenneth and Fragkiadaki, Katerina},
  journal={Advances in neural information processing systems},
  year={2024}
}

@article{murty2024bagel,
  title={BAGEL: Bootstrapping Agents by Guiding Exploration with Language},
  author={Murty, Shikhar and Manning, Christopher and Shaw, Peter and Joshi, Mandar and Lee, Kenton},
  journal={arXiv preprint arXiv:2403.08140},
  year={2024}
}

@article{zhang2024xlam,
  title={xlam: A family of large action models to empower ai agent systems},
  author={Zhang, Jianguo and Lan, Tian and Zhu, Ming and Liu, Zuxin and Hoang, Thai and Kokane, Shirley and Yao, Weiran and Tan, Juntao and Prabhakar, Akshara and Chen, Haolin and others},
  journal={arXiv preprint arXiv:2409.03215},
  year={2024}
}

@article{ou2024synatra,
  title={Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale},
  author={Ou, Tianyue and Xu, Frank F and Madaan, Aman and Liu, Jiarui and Lo, Robert and Sridhar, Abishek and Sengupta, Sudipta and Roth, Dan and Neubig, Graham and Zhou, Shuyan},
  journal={arXiv preprint arXiv:2409.15637},
  year={2024}
}

@inproceedings{hong2024cogagent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14281--14290},
  year={2024}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220101},
  year={2024},
  publisher={Springer}
}

@article{yang2024swe,
  title={Swe-agent: Agent-computer interfaces enable automated software engineering},
  author={Yang, John and Jimenez, Carlos E and Wettig, Alexander and Lieret, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
  journal={arXiv preprint arXiv:2405.15793},
  year={2024}
}

@article{xia2024agentless,
  title={Agentless: Demystifying llm-based software engineering agents},
  author={Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  journal={arXiv preprint arXiv:2407.01489},
  year={2024}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  year={2024},
  publisher={Higher Education Press Beijing}
}

@article{rahmanzadehgervi2024vision,
  title={Vision language models are blind},
  author={Rahmanzadehgervi, Pooyan and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
  journal={arXiv preprint arXiv:2407.06581},
  year={2024}
}

@article{wu2024atlas,
  title={OS-ATLAS: A Foundation Action Model for Generalist GUI Agents},
  author={Wu, Zhiyong and Wu, Zhenyu and Xu, Fangzhi and Wang, Yian and Sun, Qiushi and Jia, Chengyou and Cheng, Kanzhi and Ding, Zichen and Chen, Liheng and Liang, Paul Pu and others},
  journal={arXiv preprint arXiv:2410.23218},
  year={2024}
}

@article{aleithan2024swe,
  title={SWE-Bench+: Enhanced Coding Benchmark for LLMs},
  author={Aleithan, Reem and Xue, Haoran and Mohajer, Mohammad Mahdi and Nnorom, Elijah and Uddin, Gias and Wang, Song},
  journal={arXiv preprint arXiv:2410.06992},
  year={2024}
}

@article{zaheer2022learning,
  title={Learning to navigate wikipedia by taking random walks},
  author={Zaheer, Manzil and Marino, Kenneth and Grathwohl, Will and Schultz, John and Shang, Wendy and Babayan, Sheila and Ahuja, Arun and Dasgupta, Ishita and Kaeser-Chen, Christine and Fergus, Rob},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1529--1541},
  year={2022}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@article{hofmann2020robotic,
  title={Robotic process automation},
  author={Hofmann, Peter and Samp, Caroline and Urbach, Nils},
  journal={Electronic markets},
  volume={30},
  number={1},
  pages={99--106},
  year={2020},
  publisher={Springer}
}

@article{chen2024more,
  title={Are more llm calls all you need? towards scaling laws of compound inference systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2403.02419},
  year={2024}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@inproceedings{gurlearning,
  title={Learning to Navigate the Web},
  year=2019,
  author={Gur, Izzeddin and Rueckert, Ulrich and Faust, Aleksandra and Hakkani-Tur, Dilek},
  booktitle={International Conference on Learning Representations}
}

@article{xu2024theagentcompany,
  title={Theagentcompany: benchmarking llm agents on consequential real world tasks},
  author={Xu, Frank F and Song, Yufan and Li, Boxuan and Tang, Yuxuan and Jain, Kritanjali and Bao, Mengxue and Wang, Zora Z and Zhou, Xuhui and Guo, Zhitong and Cao, Murong and others},
  journal={arXiv preprint arXiv:2412.14161},
  year={2024}
}

@article{Yu2024ExACTTA,
  title={ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning},
  author={Xiao Yu and Baolin Peng and Vineeth Vajipey and Hao Cheng and Michel Galley and Jianfeng Gao and Zhou Yu},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.02052},
  url={https://api.semanticscholar.org/CorpusID:273098809}
}

@inproceedings{wangexecutable,
  title={Executable Code Actions Elicit Better LLM Agents},
  author={Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{qi2024webrl,
  title={WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning},
  author={Qi, Zehan and Liu, Xiao and Iong, Iat Long and Lai, Hanyu and Sun, Xueqiao and Yang, Xinyue and Sun, Jiadai and Yang, Yu and Yao, Shuntian and Zhang, Tianjie and others},
  journal={arXiv preprint arXiv:2411.02337},
  year={2024}
}

@article{putta2024agent,
  title={Agent q: Advanced reasoning and learning for autonomous ai agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  journal={arXiv preprint arXiv:2408.07199},
  year={2024}
}

@article{koh2024tree,
  title={Tree search for language model agents},
  author={Koh, Jing Yu and McAleer, Stephen and Fried, Daniel and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2407.01476},
  year={2024}
}

@article{gu2024your,
  title={Is your llm secretly a world model of the internet? model-based planning for web agents},
  author={Gu, Yu and Zheng, Boyuan and Gou, Boyu and Zhang, Kai and Chang, Cheng and Srivastava, Sanjari and Xie, Yanan and Qi, Peng and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2411.06559},
  year={2024}
}

@article{murty2024nnetscape,
  title={Nnetscape navigator: Complex demonstrations for web agents without a demonstrator},
  author={Murty, Shikhar and Bahdanau, Dzmitry and Manning, Christopher D},
  journal={arXiv preprint arXiv:2410.02907},
  year={2024}
}

@article{qinghong2024showui,
  title={ShowUI: One Vision-Language-Action Model for GUI Visual Agent},
  author={Qinghong Lin, Kevin and Li, Linjie and Gao, Difei and Yang, Zhengyuan and Wu, Shiwei and Bai, Zechen and Lei, Weixian and Wang, Lijuan and Shou, Mike Zheng},
  journal={arXiv e-prints},
  pages={arXiv--2411},
  year={2024}
}

@inproceedings{chen2024spa,
  title={Spa-bench: A comprehensive benchmark for smartphone agent evaluation},
  author={Chen, Jingxuan and Yuen, Derek and Xie, Bin and Yang, Yuhao and Chen, Gongwei and Wu, Zhihao and Yixing, Li and Zhou, Xurui and Liu, Weiwen and Wang, Shuai and others},
  booktitle={NeurIPS 2024 Workshop on Open-World Agents}
}

@article{reddy2024infogent,
  title={Infogent: An agent-based framework for web information aggregation},
  author={Reddy, Revanth Gangi and Mukherjee, Sagnik and Kim, Jeonghwan and Wang, Zhenhailong and Hakkani-Tur, Dilek and Ji, Heng},
  journal={arXiv preprint arXiv:2410.19054},
  year={2024}
}

@article{patel2024large,
  title={Large Language Models Can Self-Improve At Web Agent Tasks},
  author={Patel, Ajay and Hofmarcher, Markus and Leoveanu-Condrei, Claudiu and Dinu, Marius-Constantin and Callison-Burch, Chris and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.20309},
  year={2024}
}

@article{abuelsaad2024agent,
  title={Agent-e: From autonomous web navigation to foundational design principles in agentic systems},
  author={Abuelsaad, Tamer and Akkil, Deepak and Dey, Prasenjit and Jagmohan, Ashish and Vempaty, Aditya and Kokku, Ravi},
  journal={arXiv preprint arXiv:2407.13032},
  year={2024}
}

@article{lin2024videogui,
  title={VideoGUI: A Benchmark for GUI Automation from Instructional Videos},
  author={Lin, Kevin Qinghong and Li, Linjie and Gao, Difei and Wu, Qinchen and Yan, Mingyi and Yang, Zhengyuan and Wang, Lijuan and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2406.10227},
  year={2024}
}

@inproceedings{chen2024webvln,
  title={Webvln: Vision-and-language navigation on websites},
  author={Chen, Qi and Pitawela, Dileepa and Zhao, Chongyang and Zhou, Gengze and Chen, Hsiang-Ting and Wu, Qi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={2},
  pages={1165--1173},
  year={2024}
}

@article{chezelles2024browsergym,
  title={The BrowserGym Ecosystem for Web Agent Research},
  author={Chezelles, De and Le Sellier, Thibault and Gasse, Maxime and Lacoste, Alexandre and Drouin, Alexandre and Caccia, Massimo and Boisvert, L{\'e}o and Thakkar, Megh and Marty, Tom and Assouel, Rim and others},
  journal={arXiv preprint arXiv:2412.05467},
  year={2024}
}

@article{he2024webvoyager,
  title={WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models},
  author={He, Hongliang and Yao, Wenlin and Ma, Kaixin and Yu, Wenhao and Dai, Yong and Zhang, Hongming and Lan, Zhenzhong and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13919},
  year={2024}
}

@inproceedings{lai2024autowebglm,
  title={AutoWebGLM: A Large Language Model-based Web Navigating Agent},
  author={Lai, Hanyu and Liu, Xiao and Iong, Iat Long and Yao, Shuntian and Chen, Yuxuan and Shen, Pengbo and Yu, Hao and Zhang, Hanchen and Zhang, Xiaohan and Dong, Yuxiao and others},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5295--5306},
  year={2024}
}

@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}

@article{liu2024visualagentbench,
  title={Visualagentbench: Towards large multimodal models as visual foundation agents},
  author={Liu, Xiao and Zhang, Tianjie and Gu, Yu and Iong, Iat Long and Xu, Yifan and Song, Xixuan and Zhang, Shudan and Lai, Hanyu and Liu, Xinyi and Zhao, Hanlin and others},
  journal={arXiv preprint arXiv:2408.06327},
  year={2024}
}

@inproceedings{st2000user,
  title={The user interface as an agent environment},
  author={St. Amant, Robert and Zettlemoyer, Luke S},
  booktitle={Proceedings of the fourth international conference on Autonomous agents},
  pages={483--490},
  year={2000}
}

@inproceedings{riedl2002toward,
  title={Toward automated exploration of interactive systems},
  author={Riedl, Mark O and St. Amant, Robert},
  booktitle={Proceedings of the 7th international conference on Intelligent user interfaces},
  pages={135--142},
  year={2002}
}


@InProceedings{pmlr-v162-humphreys22a,
  title = 	 {A data-driven approach for learning to control computers},
  author =       {Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9466--9482},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/humphreys22a/humphreys22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/humphreys22a.html},
  abstract = 	 {It would be useful for machines to use computers as humans do so that they can aid us in everyday tasks. This is a setting in which there is also the potential to leverage large-scale expert demonstrations and human judgements of interactive behaviour, which are two ingredients that have driven much recent success in AI. Here we investigate the setting of computer control using keyboard and mouse, with goals specified via natural language. Instead of focusing on hand-designed curricula and specialized action spaces, we focus on developing a scalable method centered on reinforcement learning combined with behavioural priors informed by actual human-computer interactions. We achieve state-of-the-art and human-level mean performance across all tasks within the MiniWob++ benchmark, a challenging suite of computer control problems, and find strong evidence of cross-task transfer. These results demonstrate the usefulness of a unified human-agent interface when training machines to use computers. Altogether our results suggest a formula for achieving competency beyond MiniWob++ and towards controlling computers, in general, as a human would.}
}

@INPROCEEDINGS{9402046,
  author={Zheng, Yan and Liu, Yi and Xie, Xiaofei and Liu, Yepang and Ma, Lei and Hao, Jianye and Liu, Yang},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, 
  title={Automatic Web Testing Using Curiosity-Driven Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={423-435},
  keywords={Location awareness;Software as a service;Reinforcement learning;Manuals;Task analysis;Testing;Software engineering;Web testing;Reinforcement Learning;Curiosity;Exploration;Software Engineering},
  doi={10.1109/ICSE43902.2021.00048}}

@inproceedings{NEURIPS2021_21834461,
 author = {Gur, Izzeddin and Jaques, Natasha and Miao, Yingjie and Choi, Jongwook and Tiwari, Manoj  and Lee, Honglak and Faust, Aleksandra},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4157--4169},
 publisher = {Curran Associates, Inc.},
 title = {Environment Generation for Zero-Shot Compositional Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/218344619d8fb95d504ccfa11804073f-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{dean2022don,
  title={Don't Freeze Your Embedding: Lessons from Policy Finetuning in Environment Transfer},
  year=2022,
  author={Dean, Victoria and Toyama, Daniel Kenji and Precup, Doina},
  booktitle={ICLR Workshop on Agent Learning in Open-Endedness}
}

@inproceedings{li2021glider,
  title={Glider: A reinforcement learning approach to extract UI scripts from websites},
  author={Li, Yuanchun and Riva, Oriana},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1420--1430},
  year={2021}
}

@inproceedings{jiadom,
  title={DOM-Q-NET: Grounded RL on Structured Language},
  year=2019,
  author={Jia, Sheng and Kiros, Jamie Ryan and Ba, Jimmy},
  booktitle={International Conference on Learning Representations}
}

@article{iki2022berts,
  title={Do BERTs learn to use browser user interface? Exploring multi-step tasks with unified vision-and-language berts},
  author={Iki, Taichi and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2203.07828},
  year={2022}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  journal={https://www-cdn.anthropic.com},
  url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023}
}

@inproceedings{siegel2003sense,
  title={The sense-think-act paradigm revisited},
  author={Siegel, Mel},
  booktitle={1st International Workshop on Robotic Sensing, 2003. ROSE'03.},
  pages={5--pp},
  year={2003},
  organization={IEEE}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{zhang2024webpilotversatileautonomousmultiagent,
      title={WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration}, 
      author={Yao Zhang and Zijian Ma and Yunpu Ma and Zhen Han and Yu Wu and Volker Tresp},
      year={2024},
      eprint={2408.15978},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.15978}, 
}

@misc{wang2024agentworkflowmemory,
      title={Agent Workflow Memory}, 
      author={Zora Zhiruo Wang and Jiayuan Mao and Daniel Fried and Graham Neubig},
      year={2024},
      eprint={2409.07429},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.07429}, 
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}

@misc{lutz2024wilburadaptiveincontextlearning,
      title={WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents}, 
      author={Michael Lutz and Arth Bohra and Manvel Saroyan and Artem Harutyunyan and Giovanni Campagna},
      year={2024},
      eprint={2404.05902},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05902}, 
}

@article{hare2019dealing,
  title={Dealing with sparse rewards in reinforcement learning},
  author={Hare, Joshua},
  journal={arXiv preprint arXiv:1910.09281},
  year={2019}
}

@article{minsky1961steps,
  title={Steps toward artificial intelligence},
  author={Minsky, Marvin},
  journal={Proceedings of the IRE},
  volume={49},
  number={1},
  pages={8--30},
  year={1961},
  publisher={IEEE}
}

@article{reddy2018shared,
  title={Shared autonomy via deep reinforcement learning},
  author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.01744},
  year={2018}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction, Second Edition},
  author={Sutton, Richard S. and Barto, Andrew G.},
  publisher = {The MIT Press},
isbn={9780262039246},
  year={2018}
}

@article{michigan2021sneaker,
  title={Sneaker Bots \& Botnets: Malicious Digital Tools That Harm Rather than Help E-Commerce},
  author={Michigan, Sarah E},
  journal={Rutgers Bus. LJ},
  volume={17},
  pages={169},
  year={2021},
  publisher={HeinOnline}
}

@article{xiao2024towards,
  title={Towards Visual Grounding: A Survey},
  author={Xiao, Linhui and Yang, Xiaoshan and Lan, Xiangyuan and Wang, Yaowei and Xu, Changsheng},
  journal={arXiv preprint arXiv:2412.20206},
  year={2024}
}

@misc{marino2025computeruse,
        author={Marino, Kenneth and Marasović, Ana},
        title={Computer Use Survey: A Visual Survey of Computer Use Agents},
        year={2025},
        url={https://kennethmarino.com/computeruse/computeruse.html}
        }

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{xu2024crab,
  title={Crab: Cross-environment agent benchmark for multimodal language model agents},
  author={Xu, Tianqi and Chen, Linyao and Wu, Dai-Jie and Chen, Yanjun and Zhang, Zecheng and Yao, Xiang and Xie, Zhiqiang and Chen, Yongchao and Liu, Shilong and Qian, Bochen and others},
  journal={arXiv preprint arXiv:2407.01511},
  year={2024}
}

@article{sager2025comprehensive,
  title={A Comprehensive Survey of Agents for Computer Use: Foundations, Challenges, and Future Directions},
  author={Sager, Pascal J and Meyer, Benjamin and Yan, Peng and von Wartburg-Kottler, Rebekka and Etaiwi, Layan and Enayati, Aref and Nobel, Gabriel and Abdulkadir, Ahmed and Grewe, Benjamin F and Stadelmann, Thilo},
  journal={arXiv preprint arXiv:2501.16150},
  year={2025}
}

@inproceedings{hu2025agents,
  title={Os agents: A survey on mllm-based agents for computer, phone and browser use},
  author={Hu, Xueyu and Xiong, Tao and Yi, Biao and Wei, Zishu and Xiao, Ruixuan and Chen, Yurun and Ye, Jiasheng and Tao, Meiling and Zhou, Xiangxin and Zhao, Ziyu and others},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7436--7465},
  year={2025}
}

@article{yehudai2025survey,
  title={Survey on evaluation of llm-based agents},
  author={Yehudai, Asaf and Eden, Lilach and Li, Alan and Uziel, Guy and Zhao, Yilun and Bar-Haim, Roy and Cohan, Arman and Shmueli-Scheuer, Michal},
  journal={arXiv preprint arXiv:2503.16416},
  year={2025}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{grattafiori2024llama,
  title={The Llama 3 Herd of Models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{shen2024scribeagent,
  title={ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data},
  author={Shen, Junhong and Jain, Atishay and Xiao, Zedian and Amlekar, Ishan and Hadji, Mouad and Podolny, Aaron and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2411.15004},
  year={2024}
}

@article{zhang2025symbiotic,
  title={Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs},
  author={Zhang, Ruichen and Qiu, Mufan and Tan, Zhen and Zhang, Mohan and Lu, Vincent and Peng, Jie and Xu, Kaidi and Agudelo, Leandro Z and Qian, Peter and Chen, Tianlong},
  journal={arXiv preprint arXiv:2502.07942},
  year={2025}
}

@article{su2025learn,
  title={Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments},
  author={Su, Hongjin and Sun, Ruoxi and Yoon, Jinsung and Yin, Pengcheng and Yu, Tao and Ar{\i}k, Sercan {\"O}},
  journal={arXiv preprint arXiv:2501.10893},
  year={2025}
}

@article{yang2024agentoccam,
  title={AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents},
  author={Yang, Ke and Liu, Yao and Chaudhary, Sapana and Fakoor, Rasool and Chaudhari, Pratik and Karypis, George and Rangwala, Huzefa},
  journal={arXiv preprint arXiv:2410.13825},
  year={2024}
}

@misc{opencua2025,
  title={OpenCUA: Open Foundations for Computer-Use Agents},
  author={Wang, Xinyuan and Wang, Bowen and Lu, Dunjie and others},
  howpublished={\url{https://opencua.xlang.ai/}},
  year={2025}
}

@article{guo2024seed,
  title={Seed1.5-VL Technical Report},
  author={Guo, Dong and Wu, Faming and Zhu, Feida and Leng, Fuxing and Shi, Guang and Chen, Haobin and Fan, Haoqi and Wang, Jian and Jiang, Jianyu and Wang, Jiawei and others},
  journal={arXiv preprint arXiv:2505.07062},
  year={2024}
}

@misc{imagenet2009,
  title={ImageNet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009}
}

@misc{agentnetdocs,
  title={AgentNet Documentation - Annotation Requirements},
  howpublished={\url{https://junliwang.tech/agentnet-docs/requirements/annotation/annotation/}},
  year={2024}
}
@article{gur2021environment,
  title={Environment Generation for Zero-Shot Compositional Reinforcement Learning},
  author={Gur, Izzeddin and Jaques, Natasha and Miao, Kevin and Choi, Jongwook and Tomar, Manoj and Faust, Aleksandra and Nachum, Ofir},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zheng2021synergistic,
  title={Synergistic Integration of Long-Term Memory and Curiosity for Training-Free Web Agents},
  author={Zheng, Boyuan and others},
  journal={arXiv preprint},
  year={2021}
}

@article{gur2018learning,
  title={Learning to Navigate the Web},
  author={Gur, Izzeddin and Rückert, Ulrich and Faust, Aleksandra and Hakkani-Tür, Dilek},
  journal={arXiv preprint arXiv:1812.09195},
  year={2018}
}

@misc{kiela2024ai,
  title={AI Agents Are Here. What Now?},
  author={Kiela, Douwe and others},
  howpublished={\url{https://huggingface.co/blog/ethics-soc-7}},
  year={2024}
}

@article{drouin2024workarena,
  title={WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?},
  author={Drouin, Alexandre and Gasse, Maxime and Caccia, Massimo and Laradji, Issam H. and Del Verme, Manuel and Marty, Tom and Boisvert, L{\'e}o and Thakkar, Megh and Cappart, Quentin and Vazquez, David and Chapados, Nicolas and Lacoste, Alexandre},
  journal={arXiv preprint arXiv:2403.07718},
  year={2024}
}
@article{trabucco2025insta,
  title={InSTA: Towards Internet-Scale Training For Agents},
  author={Trabucco, Brandon and Sigurdsson, Gunnar and Piramuthu, Robinson and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2502.06776},
  year={2025}
}
@article{gandhi2025go,
  title={Go-Browse: Training Web Agents with Structured Exploration},
  author={Gandhi, Apurva and Neubig, Graham},
  journal={arXiv preprint arXiv:2506.03533},
  year={2025}
}
@article{wang2025inducing,
  title={Inducing programmatic skills for agentic tasks},
  author={Wang, Zora Zhiruo and Gandhi, Apurva and Neubig, Graham and Fried, Daniel},
  journal={CoLM},
  year={2025}
}
@inproceedings{gurreal,
  title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin V and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  booktitle={The Twelfth International Conference on Learning Representations}
}
@article{lai2025computerrl,
  title={Computerrl: Scaling end-to-end online reinforcement learning for computer use agents},
  author={Lai, Hanyu and Liu, Xiao and Zhao, Yanxiao and Xu, Han and Zhang, Hanchen and Jing, Bohao and Ren, Yanyu and Yao, Shuntian and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2508.14040},
  year={2025}
}
@article{li2025websailor,
  title={WebSailor: Navigating Super-human Reasoning for Web Agent},
  author={Li, Kuan and Zhang, Zhongwang and Yin, Huifeng and Zhang, Liwen and Ou, Litu and Wu, Jialong and Yin, Wenbiao and Li, Baixuan and Tao, Zhengwei and Wang, Xinyu and others},
  journal={arXiv preprint arXiv:2507.02592},
  year={2025}
}
@inproceedings{szot2025multimodal,
  title={From multimodal llms to generalist embodied agents: Methods and lessons},
  author={Szot, Andrew and Mazoure, Bogdan and Attia, Omar and Timofeev, Aleksei and Agrawal, Harsh and Hjelm, Devon and Gan, Zhe and Kira, Zsolt and Toshev, Alexander},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={10644--10655},
  year={2025}
}
@inproceedings{yang2025magma,
  title={Magma: A foundation model for multimodal ai agents},
  author={Yang, Jianwei and Tan, Reuben and Wu, Qianhui and Zheng, Ruijie and Peng, Baolin and Liang, Yongyuan and Gu, Yu and Cai, Mu and Ye, Seonghyeon and Jang, Joel and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={14203--14214},
  year={2025}
}
@inproceedings{sun2025gui,
  title={Gui-xplore: Empowering generalizable gui agents with one exploration},
  author={Sun, Yuchen and Zhao, Shanhui and Yu, Tao and Wen, Hao and Va, Samith and Xu, Mengwei and Li, Yuanchun and Zhang, Chongyang},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={19477--19486},
  year={2025}
}
@article{xiao2025ui,
  title={UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents},
  author={Xiao, Han and Wang, Guozhi and Chai, Yuxiang and Lu, Zimu and Lin, Weifeng and He, Hao and Fan, Lue and Bian, Liuyang and Hu, Rui and Liu, Liang and others},
  journal={NeurIPS},
  year={2025}
}
@article{vattikonda2025train,
  title={How to train your llm web agent: A statistical diagnosis},
  author={Vattikonda, Dheeraj and Ravichandran, Santhoshi and Penaloza, Emiliano and Nekoei, Hadi and Thakkar, Megh and de Chezelles, Thibault Le Sellier and Gontier, Nicolas and Mu{\~n}oz-M{\'a}rmol, Miguel and Shayegan, Sahar Omidi and Raimondo, Stefania and others},
  journal={arXiv preprint arXiv:2507.04103},
  year={2025}
}
@inproceedings{yangself,
  title={Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents},
  author={Yang, Qianlan and Wang, Xiangjun and Perszyk, Danielle and Wang, Yu-Xiong},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems}
}
@article{hu2025owl,
  title={Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation},
  author={Hu, Mengkang and Zhou, Yuhang and Fan, Wendong and Nie, Yuzhou and Xia, Bowei and Sun, Tao and Ye, Ziyu and Jin, Zhaoxuan and Li, Yingru and Chen, Qiguang and others},
  journal={NeurIPS},
  year={2025}
}
@article{wu2025gui,
  title={GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents},
  author={Wu, Qianhui and Cheng, Kanzhi and Yang, Rui and Zhang, Chaoyun and Yang, Jianwei and Jiang, Huiqiang and Mu, Jian and Peng, Baolin and Qiao, Bo and Tan, Reuben and others},
  journal={NeurIPS},
  year={2025}
}
@inproceedings{limobileuse,
  title={MobileUse: A Hierarchical Reflection-Driven GUI Agent for Autonomous Mobile Operation},
  author={Li, Ning and Qu, Xiangmou and Zhou, Jiamu and Wang, Jun and Wen, Muning and Du, Kounianhua and Lou, Xingyu and Peng, Qiuying and Zhang, Weinan},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems}
}
@article{ma2025autodata,
  title={AutoData: A Multi-Agent System for Open Web Data Collection},
  author={Ma, Tianyi and Qian, Yiyue and Zhang, Zheyuan and Wang, Zehong and Qian, Xiaoye and Bai, Feifan and Ding, Yifan and Luo, Xuwei and Zhang, Shinan and Murugesan, Keerthiram and others},
  journal={NeurIPS},
  year={2025}
}
@article{gou2025mind2web,
  title={Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge},
  author={Gou, Boyu and Huang, Zanming and Ning, Yuting and Gu, Yu and Lin, Michael and Qi, Weijian and Kopanev, Andrei and Yu, Botao and Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and others},
  journal={arXiv preprint arXiv:2506.21506},
  year={2025}
}
@misc{wang2025computer,
    title={Computer Agent Arena: Compare & Test Computer Use Agents on Crowdsourced Real-World Tasks},
    author={Bowen Wang and Xinyuan Wang and Jiaqi Deng and Tianbao Xie and Ryan Li and Yanzhe Zhang and Gavin Li and Toh Jing Hua and Ion Stoica and Wei-Lin Chiang and Diyi Yang and Yu Su and Yi Zhang and Zhiguo Wang and Victor Zhong and Tao Yu},
    year={2025},
}
@inproceedings{qian-etal-2025-escapebench,
    title = {E}scape{B}ench: Towards Advancing Creative Intelligence of Language Model Agents,
    author = Qian, Cheng  and
      Han, Peixuan  and
      Luo, Qinyu  and
      He, Bingxiang  and
      Chen, Xiusi  and
      Zhang, Yuji  and
      Du, Hongyi  and
      Yao, Jiarui  and
      Yang, Xiaocheng  and
      Zhang, Denghui  and
      Li, Yunzhu  and
      Ji, Heng,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.39/,
    doi = 10.18653/v1/2025.acl-long.39,
    pages = 798--820,
    ISBN = 979-8-89176-251-0,
    abstract = Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench{---}a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15{\%} average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40{\%} fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.
}
@inproceedings{xu-etal-2025-androidlab,
    title = {A}ndroid{L}ab: Training and Systematic Benchmarking of Android Autonomous Agents,
    author = Xu, Yifan  and
      Liu, Xiao  and
      Sun, Xueqiao  and
      Cheng, Siyi  and
      Yu, Hao  and
      Lai, Hanyu  and
      Zhang, Shudan  and
      Zhang, Dan  and
      Tang, Jie  and
      Dong, Yuxiao,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.107/,
    doi = 10.18653/v1/2025.acl-long.107,
    pages = 2144--2166,
    ISBN = 979-8-89176-251-0,
    abstract = Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59{\%} to 21.50{\%} for LLMs and from 1.93{\%} to 13.28{\%} for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.
}
@inproceedings{li-etal-2025-legalagentbench,
    title = {L}egal{A}gent{B}ench: Evaluating {LLM} Agents in Legal Domain,
    author = Li, Haitao  and
      Chen, Junjie  and
      Yang, Jingli  and
      Ai, Qingyao  and
      Jia, Wei  and
      Liu, Youfeng  and
      Lin, Kai  and
      Wu, Yueyue  and
      Yuan, Guozhi  and
      Hu, Yiran  and
      Wang, Wuyue  and
      Liu, Yiqun  and
      Huang, Minlie,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.116/,
    doi = 10.18653/v1/2025.acl-long.116,
    pages = 2322--2344,
    ISBN = 979-8-89176-251-0,
    abstract = With the increasing intelligence and autonomy of LLM Agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks are unable to fully capture the complexity and subtle nuances inherent in real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. To cover tasks of varying difficulty and types, we designed a scalable task construction process that enables a more precise evaluation of performance in both tool utilization and reasoning. Moreover, Beyond assessing performance through the success rate of final outcomes, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, facilitating a more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at https://github.com/CSHaitao/LegalAgentBench.
}
@inproceedings{li-etal-2025-investorbench,
    title = {INVESTORBENCH}: A Benchmark for Financial Decision-Making Tasks with {LLM}-based Agent,
    author = Li, Haohang  and
      Cao, Yupeng  and
      Yu, Yangyang  and
      Javaji, Shashidhar Reddy  and
      Deng, Zhiyang  and
      He, Yueru  and
      Jiang, Yuechen  and
      Zhu, Zining  and
      Subbalakshmi, K.p.  and
      Huang, Jimin  and
      Qian, Lingfei  and
      Peng, Xueqing  and
      Suchow, Jordan W.  and
      Xie, Qianqian,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.126/,
    doi = 10.18653/v1/2025.acl-long.126,
    pages = 2509--2525,
    ISBN = 979-8-89176-251-0,
    abstract = Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce InvestorBench, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks and cryptocurrencies, and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.
}
@inproceedings{men-etal-2025-agent,
    title = Agent-{R}eward{B}ench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents,
    author = Men, Tianyi  and
      Jin, Zhuoran  and
      Cao, Pengfei  and
      Chen, Yubo  and
      Liu, Kang  and
      Zhao, Jun,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.857/,
    doi = 10.18653/v1/2025.acl-long.857,
    pages = 17521--17541,
    ISBN = 979-8-89176-251-0,
    abstract = As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.
}
@inproceedings{ye-etal-2025-productagent,
    title = {P}roduct{A}gent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions,
    author = Ye, Jingheng  and
      Jiang, Yong  and
      Wang, Xiaobin  and
      Li, Yinghui  and
      Li, Yangning  and
      Xie, Pengjun  and
      Huang, Fei,
    editor = Potdar, Saloni  and
      Rojas-Barahona, Lina  and
      Montella, Sebastien,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track,
    month = nov,
    year = 2025,
    address = Suzhou (China),
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-industry.25/,
    doi = 10.18653/v1/2025.emnlp-industry.25,
    pages = 383--398,
    ISBN = 979-8-89176-333-3,
    abstract = Online shoppers often initiate their journey with only a vague idea of what they need, forcing them to iterate over search results until they eventually discover a suitable product. We formulate this scenario as product demand clarification: starting from an ambiguous query, an agent must iteratively ask clarifying questions, progressively refine the user{'}s intent, and retrieve increasingly relevant items. To tackle this challenge, we present **ProductAgent**, a fully autonomous conversational information-seeking agent that couples large language models with a set of domain-specific tools. ProductAgent maintains a structured memory of the dialogue, summarizes candidate products into concise feature statistics, generates strategic clarification questions, and performs retrieval over hybrid (symbolic + dense) indices in a closed decision loop. To measure real{--}world effectiveness, we further introduce **PROCLARE**, a PROduct CLArifying REtrieval benchmark that pairs ProductAgent with an LLM-driven user simulator, thereby enabling large-scale and reproducible evaluation without human annotation. On 2,000 automatically generated sessions, retrieval metrics improve monotonically with the number of turns, validating that ProductAgent captures and refines user intent through dialogue.
}
@inproceedings{lee-etal-2025-telagentbench,
    title = {T}el{A}gent{B}ench: A Multi-faceted Benchmark for Evaluating {LLM}-based Agents in Telecommunications,
    author = Lee, Sunwoo  and
      Jang, Daseong  and
      Arya, Dhammiko  and
      Han, Gyoung-eun  and
      Song, Injee  and
      Kim, SaeRom  and
      Kim, Sangjin  and
      Lee, Seojin  and
      Hong, Seokyoung  and
      Sek, Sereimony  and
      Cho, Seung-Mo  and
      Park, Sohee  and
      Yoon, Sungbin  and
      Jang, Wonbeom  and
      Davis, Eric,
    editor = Potdar, Saloni  and
      Rojas-Barahona, Lina  and
      Montella, Sebastien,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track,
    month = nov,
    year = 2025,
    address = Suzhou (China),
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-industry.83/,
    doi = 10.18653/v1/2025.emnlp-industry.83,
    pages = 1173--1211,
    ISBN = 979-8-89176-333-3,
    abstract = As Large Language Models (LLMs) evolve into powerful agentic systems, the telecommunications industry{'}s expansion into AI services necessitates industry-grounded benchmarks to evaluate their underexplored domain-specific capabilities. To address the gap left by generic benchmarks that fail to assess realistic, non-English performance, we present TelAgentBench, a Korean benchmark for the telecommunications domain evaluating five core agentic capabilities: Reasoning, Planning, Action (tool-use), Retrieval-Augmented Generation, and Instruction Following. Evaluations reveal significant performance disparities between models that employ explicit reasoning and those that do not, providing actionable insights for deploying agentic LLMs in real-world telecommunications tasks.
}
@inproceedings{qian-etal-2025-modelingagent,
    title = {M}odeling{A}gent: Bridging {LLM}s and Mathematical Modeling for Real-World Challenges,
    author = Qian, Cheng  and
      Du, Hongyi  and
      Wang, Hongru  and
      Chen, Xiusi  and
      Zhang, Yuji  and
      Sil, Avirup  and
      Zhai, ChengXiang  and
      McKeown, Kathleen  and
      Ji, Heng,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Findings of the Association for Computational Linguistics: EMNLP 2025,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-emnlp.85/,
    doi = 10.18653/v1/2025.findings-emnlp.85,
    pages = 1599--1633,
    ISBN = 979-8-89176-335-7,
    abstract = Recent progress in large language models (LLMs) has enabled substantial advances in solving mathematical problems. However, existing benchmarks often fail to reflect real-world complexity, which demand open-ended, interdisciplinary reasoning and integration of computational tools. To address this gap, we introduce **ModelingBench**, a novel benchmark featuring real-world-inspired, open-ended problems from math modeling competitions across diverse domains, ranging from urban traffic optimization to ecosystem resource planning. These tasks require translating natural language into formal mathematical formulations, applying appropriate tools, and producing structured, defensible reports. ModelingBench supports multiple valid solutions, capturing the ambiguity and creativity of practical modeling. To solve these challenges, we present **ModelingAgent**, a multi-agent framework that coordinates tool use, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. Empirical results show that ModelingAgent substantially outperforms strong baselines and often produces solutions indistinguishable from those of human experts. Together, our work provides a comprehensive framework for evaluating and advancing real-world problem-solving in open-ended, interdisciplinary modeling challenges. All the codes are released for future research.
}
@inproceedings{kulkarni-etal-2025-massive,
    title = {MASSIVE}-Agents: A Benchmark for Multilingual Function-Calling in 52 Languages,
    author = Kulkarni, Mayank  and
      Mazzia, Vittorio  and
      Gaspers, Judith  and
      Hench, Chris  and
      FitzGerald, Jack,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Findings of the Association for Computational Linguistics: EMNLP 2025,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-emnlp.1099/,
    doi = 10.18653/v1/2025.findings-emnlp.1099,
    pages = 20193--20215,
    ISBN = 979-8-89176-335-7,
    abstract = We present MASSIVE-Agents, a new benchmark for assessing multilingual function calling across 52 languages. We created MASSIVE-Agents by cleaning the original MASSIVE dataset and then reformatting it for evaluation within the Berkeley Function-Calling Leaderboard (BFCL) framework. The full benchmark comprises 47,020 samples with an average of 904 samples per language, covering 55 different functions and 286 arguments. We benchmarked 21 models using Amazon Bedrock and present the results along with associated analyses. MASSIVE-Agents is challenging, with the top model Nova Premier achieving an average Abstract Syntax Tree (AST) Accuracy of 34.05{\%} across all languages, with performance varying significantly from 57.37{\%} for English to as low as 6.81{\%} for Amharic. Some models, particularly smaller ones, yielded a score of zero for the more difficult languages. Additionally, we provide results from ablations using a custom 1-shot prompt, ablations with prompts translated into different languages, and comparisons based on model latency.
}
@inproceedings{zhou-etal-2025-rulearena,
    title = {R}ule{A}rena: A Benchmark for Rule-Guided Reasoning with {LLM}s in Real-World Scenarios,
    author = Zhou, Ruiwen  and
      Hua, Wenyue  and
      Pan, Liangming  and
      Cheng, Sitao  and
      Wu, Xiaobao  and
      Yu, En  and
      Wang, William Yang,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.27/,
    doi = 10.18653/v1/2025.acl-long.27,
    pages = 550--572,
    ISBN = 979-8-89176-251-0,
    abstract = This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains {--} airline baggage fees, NBA transactions, and tax regulations {--} RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/rulearena.
}
@inproceedings{xu-etal-2025-turkingbench,
    title = {T}urking{B}ench: A Challenge Benchmark for Web Agents,
    author = Xu, Kevin  and
      Kordi, Yeganeh  and
      Nayak, Tanay  and
      Asija, Adi  and
      Wang, Yizhong  and
      Sanders, Kate  and
      Byerly, Adam  and
      Zhang, Jingyu  and
      Van Durme, Benjamin  and
      Khashabi, Daniel,
    editor = Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu,
    booktitle = Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),
    month = apr,
    year = 2025,
    address = Albuquerque, New Mexico,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.naacl-long.188/,
    doi = 10.18653/v1/2025.naacl-long.188,
    pages = 3694--3710,
    ISBN = 979-8-89176-189-6,
    abstract = Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments.Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally designed for crowdsourcing workers to perform various annotation tasks. Each task{'}s HTML instructions are instantiated with different values derived from crowdsourcing tasks, creating diverse instances. This benchmark includes 32.2K instances spread across 158 tasks.To support the evaluation of TurkingBench, we have developed a framework that links chatbot responses to actions on web pages (e.g., modifying a text box, selecting a radio button). We assess the performance of cutting-edge private and open-source models, including language-only and vision-language models (such as GPT4 and InternVL), on this benchmark. Our results show that while these models outperform random chance, there is still significant room for improvement. We hope that this benchmark will drive progress in the evaluation and development of web-based agents.
}
@inproceedings{ahn-etal-2025-flashadventure,
    title = {F}lash{A}dventure: A Benchmark for {GUI} Agents Solving Full Story Arcs in Diverse Adventure Games,
    author = Ahn, Jaewoo  and
      Kim, Junseo  and
      Yun, Heeseung  and
      Son, Jaehyeon  and
      Park, Dongmin  and
      Cho, Jaewoong  and
      Kim, Gunhee,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-main.1192/,
    doi = 10.18653/v1/2025.emnlp-main.1192,
    pages = 23365--23395,
    ISBN = 979-8-89176-332-6,
    abstract = GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap{---}the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.
}
@inproceedings{hu-etal-2025-compileagent,
    title = {C}ompile{A}gent: Automated Real-World Repo-Level Compilation with Tool-Integrated {LLM}-based Agent System,
    author = Hu, Li  and
      Chen, Guoqiang  and
      Shang, Xiuwei  and
      Cheng, Shaoyin  and
      Wu, Benlong  and
      LiGangyang, LiGangyang  and
      Zhu, Xu  and
      Zhang, Weiming  and
      Yu, Nenghai,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.103/,
    doi = 10.18653/v1/2025.acl-long.103,
    pages = 2078--2091,
    ISBN = 979-8-89176-251-0,
    abstract = With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10{\%} to 71{\%}. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects. The complete code and data are available at https://github.com/Ch3nYe/AutoCompiler.
}
@inproceedings{zhu-etal-2025-multiagentbench,
    title = {M}ulti{A}gent{B}ench : Evaluating the Collaboration and Competition of {LLM} agents,
    author = Zhu, Kunlun  and
      Du, Hongyi  and
      Hong, Zhaochen  and
      Yang, Xiaocheng  and
      Guo, Shuyi  and
      Wang, Zhe  and
      Wang, Zhenhailong  and
      Qian, Cheng  and
      Tang, Robert  and
      Ji, Heng  and
      You, Jiaxuan,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.421/,
    doi = 10.18653/v1/2025.acl-long.421,
    pages = 8580--8622,
    ISBN = 979-8-89176-251-0,
    abstract = Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents; yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, cognitive planning improves milestone achievement rates by 3{\%}. Code and dataset will be made publicly available. Code and datasets are publicavailable at https://github.com/ulab-uiuc/MARBLE
}
@inproceedings{wang-etal-2025-x,
    title = {X}-{W}eb{A}gent{B}ench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System,
    author = Wang, Peng  and
      Tao, Ruihan  and
      Chen, Qiguang  and
      Hu, Mengkang  and
      Qin, Libo,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Findings of the Association for Computational Linguistics: ACL 2025,
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-acl.988/,
    doi = 10.18653/v1/2025.findings-acl.988,
    pages = 19320--19335,
    ISBN = 979-8-89176-256-5,
    abstract = Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.
}
@inproceedings{hu-etal-2025-repro,
    title = {REPRO}-Bench: Can Agentic {AI} Systems Assess the Reproducibility of Social Science Research?,
    author = Hu, Chuxuan  and
      Zhang, Liyun  and
      Lim, Yeji  and
      Wadhwani, Aum  and
      Peters, Austin  and
      Kang, Daniel,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Findings of the Association for Computational Linguistics: ACL 2025,
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-acl.1210/,
    doi = 10.18653/v1/2025.findings-acl.1210,
    pages = 23616--23626,
    ISBN = 979-8-89176-256-5,
    abstract = Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4{\%}. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71{\%}. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.
}
@inproceedings{liu-etal-2025-projecteval,
    title = {P}roject{E}val: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation,
    author = Liu, Kaiyuan  and
      Pan, Youcheng  and
      Xiang, Yang  and
      He, Daojing  and
      Li, Jing  and
      Du, Yexing  and
      Gao, Tianrun,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Findings of the Association for Computational Linguistics: ACL 2025,
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-acl.1036/,
    doi = 10.18653/v1/2025.findings-acl.1036,
    pages = 20205--20221,
    ISBN = 979-8-89176-256-5,
    abstract = Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation{'}s automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.
}
@inproceedings{gioacchini-etal-2025-autopenbench,
    title = {A}uto{P}en{B}ench: A Vulnerability Testing Benchmark for Generative Agents,
    author = Gioacchini, Luca  and
      Delsanto, Alexander  and
      Drago, Idilio  and
      Mellia, Marco  and
      Siracusano, Giuseppe  and
      Bifulco, Roberto,
    editor = Potdar, Saloni  and
      Rojas-Barahona, Lina  and
      Montella, Sebastien,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track,
    month = nov,
    year = 2025,
    address = Suzhou (China),
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-industry.114/,
    doi = 10.18653/v1/2025.emnlp-industry.114,
    pages = 1615--1624,
    ISBN = 979-8-89176-333-3,
    abstract = LLM agents show promise for vulnerability testing. We however lack benchmarks to evaluate and compare solutions. AutoPenBench covers this need offering an open benchmark for the evaluation of vulnerability testing agents. It includes 33 tasks, ranging from introductory exercises to actual vulnerable systems. It supports MCP, enabling the comparison of agent capabilities. We introduce milestones per task, allowing the comparison of intermediate steps where agents struggle. To illustrate the use of AutoPenBench we evaluate autonomous and human-assisted agent architectures. The former achieves 21{\%} success rates, insufficient for production, while human-assisted agents reach 64{\%} success, indicating a viable industrial path. AutoPenBench is offered as open source and enables fair comparison of agents.
}
@inproceedings{wang-etal-2025-ecom,
    title = {EC}om-Bench: Can {LLM} Agent Resolve Real-World {E}-commerce Customer Support Issues?,
    author = Wang, Haoxin  and
      Peng, Xianhan  and
      Cheng, Huang  and
      Huang, Yizhe  and
      Gong, Ming  and
      Yang, Chenghan  and
      Liu, Yang  and
      Lin, Jiang,
    editor = Potdar, Saloni  and
      Rojas-Barahona, Lina  and
      Montella, Sebastien,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track,
    month = nov,
    year = 2025,
    address = Suzhou (China),
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-industry.19/,
    doi = 10.18653/v1/2025.emnlp-industry.19,
    pages = 276--284,
    ISBN = 979-8-89176-333-3,
    abstract = In this paper, we introduce , the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making highly challenging. For instance, even advanced models like GPT-4o achieve only a 10{--}20{\%} pass3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at \url{https://github.com/XiaoduoAILab/ECom-Bench} to facilitate further research and development in this domain.
}
@inproceedings{shen-shen-2025-auto,
    title = Auto-{SLURP}: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant,
    author = Shen, Lei  and
      Shen, Xiaoyu,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Findings of the Association for Computational Linguistics: EMNLP 2025,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-emnlp.596/,
    doi = 10.18653/v1/2025.findings-emnlp.596,
    pages = 11163--11174,
    ISBN = 979-8-89176-335-7,
    abstract = In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of smart personal assistants. Auto-SLURP extends the original SLURP dataset{---}initially developed for natural language understanding tasks{---}by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress.
}
@inproceedings{pavel-etal-2025-swe,
    title = {SWE}-{MERA}: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks,
    author = Pavel, Adamenko  and
      Mikhail, Ivanov  and
      Valeev, Aidar  and
      Levichev, Rodion  and
      Zadorozhny, Pavel  and
      Lopatin, Ivan  and
      Babaev, Dmitrii  and
      Fenogenova, Alena  and
      Malykh, Valentin,
    editor = {Habernal, Ivan  and
      Schulam, Peter  and
      Tiedemann, J{\o}rg},
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-demos.30/,
    doi = 10.18653/v1/2025.emnlp-demos.30,
    pages = 440--452,
    ISBN = 979-8-89176-334-0,
    abstract = The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset. Recent studies have uncovered severe data contamination issues, e.g., SWE-bench reports 32.67{\%} of successful patches involve direct solution leakage and 31.08{\%} pass due to inadequate test cases. We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation. Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 728 samples currently available. Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models. We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.
}
@inproceedings{sun-etal-2025-collab,
    title = Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents,
    author = Sun, Haochen  and
      Zhang, Shuwen  and
      Niu, Lujie  and
      Ren, Lei  and
      Xu, Hao  and
      Fu, Hao  and
      Zhao, Fangkun  and
      Yuan, Caixia  and
      Wang, Xiaojie,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-main.249/,
    doi = 10.18653/v1/2025.emnlp-main.249,
    pages = 4922--4951,
    ISBN = 979-8-89176-332-6,
    abstract = Large Language Models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-based Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks in two novel ways. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments with 13 popular LLMs and show that, while the LLMs exhibit a strong ability in goal interpretation, there are significant shortcomings in active collaboration and continuous adaptation, which are critical for efficiently fulfilling complex tasks. Notably, we highlight the strengths and weaknesses of LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-source benchmark. The environments, 30 open-ended tasks, and the evaluation package are publicly available at https://github.com/YusaeMeow/Collab-Overcooked.
}
@inproceedings{yan-etal-2025-lmr,
title = {LMR}-{BENCH}: Evaluating {LLM} Agent{'}s Ability on Reproducing Language Modeling Research,
author = Yan, Shuo  and
Li, Ruochen  and
Luo, Ziming  and
Wang, Zimu  and
Li, Daoyang  and
Jing, Liqiang  and
He, Kaiyu  and
Wu, Peilin  and
Ni, Juntong  and
Michalopoulos, George  and
Zhang, Yue  and
Zhang, Ziyang  and
Zhang, Mian  and
Chen, Zhiyu  and
Du, Xinya,
editor = Christodoulopoulos, Christos  and
Chakraborty, Tanmoy  and
Rose, Carolyn  and
Peng, Violet,
booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,
month = nov,
year = 2025,
address = Suzhou, China,
publisher = Association for Computational Linguistics,
url = https://aclanthology.org/2025.emnlp-main.314/,
doi = 10.18653/v1/2025.emnlp-main.314,
pages = 6175--6197,
ISBN = 979-8-89176-332-6,
abstract = Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research.
}
@inproceedings{wang-etal-2025-fedmabench,
    title = {F}ed{MAB}ench: Benchmarking Mobile {GUI} Agents on Decentralized Heterogeneous User Data,
    author = Wang, WenHao  and
      Yu, Zijie  and
      Ye, Rui  and
      Zhang, Jianqing  and
      Liu, Guangyi  and
      Liu, Liang  and
      Chen, Siheng  and
      Wang, Yanfeng,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.emnlp-main.1341/,
    doi = 10.18653/v1/2025.emnlp-main.1341,
    pages = 26398--26419,
    ISBN = 979-8-89176-332-6,
    abstract = Mobile GUI agents have attracted tremendous research participation recently. Traditional approaches to mobile agent training rely on centralized data collection, leading to high cost and limited scalability. Distributed training utilizing federated learning offers an alternative by harnessing real-world user data, providing scalability and reducing costs. However, pivotal challenges, including the absence of standardized benchmarks, hinder progress in this field. To tackle the challenges, we introduce FedMABench, the first benchmark for federated training and evaluation of mobile GUI agents, specifically designed for heterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8 federated algorithms, 10+ base models, and over 800 apps across 5 categories, providing a comprehensive framework for evaluating mobile agents across diverse environments. Through extensive experiments, we uncover several key insights: federated algorithms consistently outperform local training; the distribution of specific apps plays a crucial role in heterogeneity; and, even apps from distinct categories can exhibit correlations during training. FedMABench is publicly available at: https://github.com/wwh0411/FedMABench.
}
@inproceedings{lindenbauer-etal-2025-gitgoodbench,
    title = {G}it{G}ood{B}ench: A Novel Benchmark For Evaluating Agentic Performance On Git,
    author = Lindenbauer, Tobias  and
      Bogomolov, Egor  and
      Zharov, Yaroslav,
    editor = Kamalloo, Ehsan  and
      Gontier, Nicolas  and
      Lu, Xing Han  and
      Dziri, Nouha  and
      Murty, Shikhar  and
      Lacoste, Alexandre,
    booktitle = Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.realm-1.19/,
    doi = 10.18653/v1/2025.realm-1.19,
    pages = 272--288,
    ISBN = 979-8-89176-264-0,
    abstract = Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on Version Control System (VCS) tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11{\%} solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming.
}
@inproceedings{
zhang2025defenderbench,
title={DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments},
author={Chiyu Zhang and Marc-Alexandre C{\^o}t{\'e} and Michael Albada and Anush Sankaran and Jack W Stokes and Tong Wang and Amir H. Abdi and William Blum and Muhammad Abdul-Mageed},
booktitle={NeurIPS Workshop on Scaling Environments for Agents},
year={2025},
url={https://openreview.net/forum?id=BtZBwwjO5d}
}
@inproceedings{kang-xiong-2025-researcharena,
    title = {R}esearch{A}rena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents,
    author = Kang, Hao  and
      Xiong, Chenyan,
    editor = Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet,
    booktitle = Findings of the Association for Computational Linguistics: EMNLP 2025,
    month = nov,
    year = 2025,
    address = Suzhou, China,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-emnlp.303/,
    doi = 10.18653/v1/2025.findings-emnlp.303,
    pages = 5653--5671,
    ISBN = 979-8-89176-335-7,
    abstract = Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys{---}a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance. These results underscore significant opportunities for advancing LLMs in autonomous research. We open-source the code to construct the ResearchArena benchmark at https://github.com/cxcscmu/ResearchArena.
}
@inproceedings{xu-etal-2025-crab,
    title = {CRAB}: Cross-environment Agent Benchmark for Multimodal Language Model Agents,
    author = Xu, Tianqi  and
      Chen, Linyao  and
      Wu, Dai-Jie  and
      Chen, Yanjun  and
      Zhang, Zecheng  and
      Yao, Xiang  and
      Xie, Zhiqiang  and
      Chen, Yongchao  and
      Liu, Shilong  and
      Qian, Bochen  and
      Yang, Anjie  and
      Jin, Zhaoxuan  and
      Deng, Jianbo  and
      Torr, Philip  and
      Ghanem, Bernard  and
      Li, Guohao,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Findings of the Association for Computational Linguistics: ACL 2025,
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-acl.1113/,
    doi = 10.18653/v1/2025.findings-acl.1113,
    pages = 21607--21647,
    ISBN = 979-8-89176-256-5,
    abstract = The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and thecomplexities of constructing tasks and evaluators. To overcome these limitations, we introduce CRAB, the first cross-environment agent benchmark framework, incorporating a graph-based fine-grained evaluation method and an efficient task generation method. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging CRAB, we develope CRAB Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated 6 advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01{\%}.
}
@inproceedings{wolflein-etal-2025-llm,
    title = {LLM} Agents Making Agent Tools,
    author = {W{\o}lflein, Georg  and
      Ferber, Dyke  and
      Truhn, Daniel  and
      Arandjelovic, Ognjen  and
      Kather, Jakob Nikolas},
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.1266/,
    doi = 10.18653/v1/2025.acl-long.1266,
    pages = 26092--26130,
    ISBN = 979-8-89176-251-0,
    abstract = Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80{\%} of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.
}
@inproceedings{xi-etal-2025-agentgym,
    title = {A}gent{G}ym: Evaluating and Training Large Language Model-based Agents across Diverse Environments,
    author = Xi, Zhiheng  and
      Ding, Yiwen  and
      Chen, Wenxiang  and
      Hong, Boyang  and
      Guo, Honglin  and
      Wang, Junzhe  and
      Guo, Xin  and
      Yang, Dingwen  and
      Liao, Chenyang  and
      He, Wei  and
      Gao, Songyang  and
      Chen, Lu  and
      Zheng, Rui  and
      Zou, Yicheng  and
      Gui, Tao  and
      Zhang, Qi  and
      Qiu, Xipeng  and
      Huang, Xuanjing  and
      Wu, Zuxuan  and
      Jiang, Yu-Gang,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.1355/,
    doi = 10.18653/v1/2025.acl-long.1355,
    pages = 27914--27961,
    ISBN = 979-8-89176-251-0,
    abstract = Large language models (LLMs) have emerged as a promising foundation to build generally-capable agents (LLM-based agents) that can handle multi-turn decision-making tasks across various environments. However, the community lacks a unified interactive framework that covers diverse environments for comprehensive evaluation of agents, and enables exploration and learning for their self-improvement. To address this, we propose AgentGym, a framework featuring 7 real-world scenarios, 14 environments, and 89 tasks for unified, real-time, and concurrent agent interaction. We construct expanded instruction set, high-quality trajectories, and comprehensive benchmarking suite for developing LLM-based agents. Moreover, AgentGym supports interactive exploration and learning for agents through multi-turn interactions and real-time feedback. Based on AgentGym, we take the initial step to develop LLM-based agents that can handle diverse tasks via methods like self-improvement or reinforcement learning. Experimental results show that the trained agents can achieve results comparable to commercial models. We hope our work can help the community develop more advanced LLM-based agents. We release the code, dataset, benchmark, and checkpoints at https://agentgym.github.io/.
}
@inproceedings{liao-etal-2025-reflectool,
    title = {R}eflec{T}ool: Towards Reflection-Aware Tool-Augmented Clinical Agents,
    author = Liao, Yusheng  and
      Jiang, Shuyang  and
      Wang, Yanfeng  and
      Wang, Yu,
    editor = Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher,
    booktitle = Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    month = jul,
    year = 2025,
    address = Vienna, Austria,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.acl-long.663/,
    doi = 10.18653/v1/2025.acl-long.663,
    pages = 13507--13531,
    ISBN = 979-8-89176-251-0,
    abstract = Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench (CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflectTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflectTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods{--}iterative refinement and candidate selection. Extensive experiments on CAB demonstrate that ReflectTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks. Our code and datasets are available at https://github.com/BlueZeros/ReflecTool.
}
@inproceedings{wang-etal-2025-cve,
    title = {CVE}-Bench: Benchmarking {LLM}-based Software Engineering Agent{'}s Ability to Repair Real-World {CVE} Vulnerabilities,
    author = Wang, Peiran  and
      Liu, Xiaogeng  and
      Xiao, Chaowei,
    editor = Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu,
    booktitle = Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),
    month = apr,
    year = 2025,
    address = Albuquerque, New Mexico,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.naacl-long.212/,
    doi = 10.18653/v1/2025.naacl-long.212,
    pages = 4207--4224,
    ISBN = 979-8-89176-189-6,
    abstract = Automated vulnerability repair is a crucial field within software engineering and security research. Large Language Models (LLMs) and LLM agents have demonstrated significant potential in this domain by understanding descriptions in natural language and generating corresponding formal code. Although the coding capabilities of LLMs have advanced rapidly, evaluation benchmarks for real-world programming setups are still lagging, preventing the development of LLM and LLM agents in real-world vulnerability repair. To this end, we introduce CVE-Bench, an evaluation framework consisting of 509 Common Vulnerabilities and Exposures (CVEs) from four programming languages and 120 popular open-source repositories. Unlike previous vulnerability repair benchmarks, which only involve the code input and output, we provide LLM agents with a test environment that simulates the real-world vulnerability repair process. This environment provides multiple levels of CVE information modeling, such as black-box testing and white-box testing. It enables the agents to use static analysis tools to assist their repair process. Our evaluation reveals that the SWE-agent can only repair 21{\%} of vulnerabilities at its best. Furthermore, they lack expert knowledge about how to use the analysis tool to assist in vulnerability repair.
}
@inproceedings{xiao-etal-2025-csr,
    title = {CSR}-Bench: Benchmarking {LLM} Agents in Deployment of Computer Science Research Repositories,
    author = Xiao, Yijia  and
      Wang, Runhui  and
      Kong, Luyang  and
      Golac, Davor  and
      Wang, Wei,
    editor = Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu,
    booktitle = Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),
    month = apr,
    year = 2025,
    address = Albuquerque, New Mexico,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.naacl-long.633/,
    doi = 10.18653/v1/2025.naacl-long.633,
    pages = 12705--12723,
    ISBN = 979-8-89176-189-6,
    abstract = The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.
}
@inproceedings{huang-etal-2025-crmarena,
    title = {CRMA}rena: Understanding the Capacity of {LLM} Agents to Perform Professional {CRM} Tasks in Realistic Environments,
    author = Huang, Kung-Hsiang  and
      Prabhakar, Akshara  and
      Dhawan, Sidharth  and
      Mao, Yixin  and
      Wang, Huan  and
      Savarese, Silvio  and
      Xiong, Caiming  and
      Laban, Philippe  and
      Wu, Chien-Sheng,
    editor = Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu,
    booktitle = Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),
    month = apr,
    year = 2025,
    address = Albuquerque, New Mexico,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.naacl-long.194/,
    doi = 10.18653/v1/2025.naacl-long.194,
    pages = 3830--3850,
    ISBN = 979-8-89176-189-6,
    abstract = Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 58{\%} of the tasks with ReAct prompting, and less than 65{\%} even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments.
}
@inproceedings{yin-etal-2025-mmau,
    title = {MMAU}: A Holistic Benchmark of Agent Capabilities Across Diverse Domains,
    author = Yin, Guoli  and
      Bai, Haoping  and
      Ma, Shuang  and
      Nan, Feng  and
      Sun, Yanchao  and
      Xu, Zhaoyang  and
      Ma, Shen  and
      Lu, Jiarui  and
      Kong, Xiang  and
      Zhang, Aonan  and
      Yap, Dian Ang  and
      Zhang, Yizhe  and
      Ahnert, Karsten  and
      Kamath, Vik  and
      Berglund, Mathias  and
      Walsh, Dominic  and
      Gindele, Tobias  and
      Wiest, Juergen  and
      Lai, Zhengfeng  and
      Wang, Xiaoming Simon  and
      Shan, Jiulong  and
      Cao, Meng  and
      Pang, Ruoming  and
      Wang, Zirui,
    editor = Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu,
    booktitle = Findings of the Association for Computational Linguistics: NAACL 2025,
    month = apr,
    year = 2025,
    address = Albuquerque, New Mexico,
    publisher = Association for Computational Linguistics,
    url = https://aclanthology.org/2025.findings-naacl.267/,
    doi = 10.18653/v1/2025.findings-naacl.267,
    pages = 4737--4765,
    ISBN = 979-8-89176-195-7,
    abstract = Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluate models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covering five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 20 representative models on MMAU, we provide deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance.
}
@article{song2025bearcubs,
  title={Bearcubs: A benchmark for computer-using web agents},
  author={Song, Yixiao and Thai, Katherine and Pham, Chau Minh and Chang, Yapei and Nadaf, Mazin and Iyyer, Mohit},
  journal={CoLM},
  year={2025}
}
@article{yao2022webshop,
  title={WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in neural information processing systems},
  year={2022}
}
@inproceedings{liuvisualagentbench,
  title={VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents},
  author={Liu, Xiao and Zhang, Tianjie and Gu, Yu and Iong, Iat Long and XiXuan, Song and Xu, Yifan and Zhang, Shudan and Lai, Hanyu and Sun, Jiadai and Yang, Xinyue and others},
  booktitle={The Thirteenth International Conference on Learning Representations}
}
@inproceedings{luomcp,
  title={MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers},
  author={Luo, Ziyang and Shen, Zhiqi and Yang, Wenzhuo and Zhao, Zirui and Jwalapuram, Prathyusha and Saha, Amrita and Sahoo, Doyen and Savarese, Silvio and Xiong, Caiming and Li, Junnan},
  booktitle={Workshop on Scaling Environments for Agents}
}
@article{wei2025browsecomp,
  title={Browsecomp: A simple yet challenging benchmark for browsing agents},
  author={Wei, Jason and Sun, Zhiqing and Papay, Spencer and McKinney, Scott and Han, Jeffrey and Fulford, Isa and Chung, Hyung Won and Passos, Alex Tachard and Fedus, William and Glaese, Amelia},
  journal={arXiv preprint arXiv:2504.12516},
  year={2025}
}
@article{xu2025web,
  title={Web-bench: A llm code benchmark based on web standards and frameworks},
  author={Xu, Kai and Mao, YiWei and Guan, XinYi and Feng, ZiLong},
  journal={arXiv preprint arXiv:2505.07473},
  year={2025}
}
@article{wang2025odysseybench,
  title={Odysseybench: Evaluating llm agents on long-horizon complex office application workflows},
  author={Wang, Weixuan and Han, Dongge and Diaz, Daniel Madrigal and Xu, Jin and R{\u}hle, Victor and Rajmohan, Saravan},
  journal={arXiv preprint arXiv:2508.09124},
  year={2025}
}
@inproceedings{wuwebwalker,
  title={WebWalker: Benchmarking LLMs in Web Traversal},
  author={Wu, Jialong and Yin, Wenbiao and Jiang, Yong and Wang, Zhenglin and Xi, Zekun and Fang, Runnan and Zhang, Linhai and He, Yulan and Zhou, Deyu and Xie, Pengjun and others},
  booktitle={Workshop on Reasoning and Planning for Large Language Models}
}
@article{webchorearenawebchorearena,
  title={WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks},
  author={WebChoreArena, WebArena}
}
@article{bragg2025astabench,
  title={AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite},
  author={Bragg, Jonathan and D'Arcy, Mike and Balepur, Nishant and Bareket, Dan and Dalvi, Bhavana and Feldman, Sergey and Haddad, Dany and Hwang, Jena D and Jansen, Peter and Kishore, Varsha and others},
  journal={arXiv preprint arXiv:2510.21652},
  year={2025}
}
@article{li2025mm,
  title={Mm-browsecomp: A comprehensive benchmark for multimodal browsing agents},
  author={Li, Shilong and Bu, Xingyuan and Wang, Wenjie and Liu, Jiaheng and Dong, Jun and He, Haoyang and Lu, Hao and Zhang, Haozhe and Jing, Chenchen and Li, Zhen and others},
  journal={arXiv preprint arXiv:2508.13186},
  year={2025}
}
@article{peeters2025webmall,
  title={WebMall--A Multi-Shop Benchmark for Evaluating Web Agents},
  author={Peeters, Ralph and Steiner, Aaron and Schwarz, Luca and Caspary, Julian Yuya and Bizer, Christian},
  journal={arXiv preprint arXiv:2508.13024},
  year={2025}
}
@inproceedings{jang2025scalable,
  title={Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents},
  author={Jang, Yunseok and Song, Yeda and Sohn, Sungryull and Logeswaran, Lajanugen and Luo, Tiange and Kim, Dong-Ki and Bae, Kyunghoon and Lee, Honglak},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={8604--8614},
  year={2025}
}
@inproceedings{huang2025spiritsight,
  title={Spiritsight agent: Advanced gui agent with one look},
  author={Huang, Zhiyuan and Cheng, Ziming and Pan, Junting and Hou, Zhaohui and Zhan, Mingjie},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={29490--29500},
  year={2025}
}
@article{yang2025macosworld,
  title={macOSWorld: A Multilingual Interactive Benchmark for GUI Agents},
  author={Yang, Pei and Ci, Hai and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2506.04135},
  year={2025}
}
@article{luo2025open,
  title={Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents},
  author={Luo, Yaxin and Li, Zhaoyi and Liu, Jiacheng and Cui, Jiacheng and Zhao, Xiaohan and Shen, Zhiqiang},
  journal={NeurIPS},
  year={2025}
}
@article{hong2025embodied,
  title={Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence},
  author={Hong, Yining and Sun, Rui and Li, Bingxuan and Yao, Xingcheng and Wu, Maxine and Chien, Alexander and Yin, Da and Wu, Ying Nian and Wang, Zhecan James and Chang, Kai-Wei},
  journal={NeurIPS},
  year={2025}
}
@article{garg2025real,
  title={Real: Benchmarking autonomous agents on deterministic simulations of real websites},
  author={Garg, Divyansh and VanWeelden, Shaun and Caples, Diego and Draguns, Andis and Ravi, Nikil and Putta, Pranav and Garg, Naman and Abraham, Tomas and Lara, Michael and Lopez, Federico and others},
  journal={arXiv preprint arXiv:2504.11543},
  year={2025}
}
@article{kapoor2025holistic,
  title={Holistic agent leaderboard: The missing infrastructure for ai agent evaluation},
  author={Kapoor, Sayash and Stroebl, Benedikt and Kirgis, Peter and Nadgir, Nitya and Siegel, Zachary S and Wei, Boyi and Xue, Tianci and Chen, Ziru and Chen, Felix and Utpala, Saiteja and others},
  journal={arXiv preprint arXiv:2510.11977},
  year={2025}
}
@article{zhu2025establishing,
  title={Establishing best practices for building rigorous agentic benchmarks},
  author={Zhu, Yuxuan and Jin, Tengjun and Pruksachatkun, Yada and Zhang, Andy and Liu, Shu and Cui, Sasha and Kapoor, Sayash and Longpre, Shayne and Meng, Kevin and Weiss, Rebecca and others},
  journal={arXiv preprint arXiv:2507.02825},
  year={2025}
}
@misc{gilley2025bitterlesson,
  author       = {Jasper Gilley and {The Yutori Team}},
  title        = {The Bitter Lesson for Web Agents},
  howpublished = {\url{https://yutori.com/blog/the-bitter-lesson-for-web-agents}},
  month        = nov,
  year         = {2025},
  note         = {Yutori Blog. Accessed: 2024-12-05}
}
@misc{browseruse2024,
  title        = {Browser Use - The AI Browser Agent},
  author       = {{Browser Use}},
  year         = {2024},
  howpublished = {\url{https://browser-use.com/}},
  note         = {Accessed: 2024-12-05}
}
@misc{pichai2025gemini3,
  author       = {{Gemini Team}},
  title        = {A New Era of Intelligence with Gemini 3},
  howpublished = {\url{https://blog.google/products/gemini/gemini-3/}},
  month        = nov,
  year         = {2025},
  note         = {Google Blog, The Keyword. Accessed: 2024-12-05},
  organization = {Google}
}
@misc{yutori2025navigator,
  author       = {The Yutori Team},
  title        = {Introducing Navigator},
  howpublished = {\url{https://yutori.com/blog/introducing-navigator}},
  note         = {Yutori Blog},
  year         = {2025},
}
@misc{cohere2025agentstudio,
  author       = {{Cohere}},
  title        = {Automate and Accelerate Work with AI Agents},
  howpublished = {\url{https://cohere.com/north/agent-studio}},
  year         = {2025},
  note         = {North Agent Studio. Accessed: 2024-12-05},
  organization = {Cohere}
}

